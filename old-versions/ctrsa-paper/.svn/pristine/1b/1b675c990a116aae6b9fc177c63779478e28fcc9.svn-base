\section{Hardware Design} \label{sec:design}

Our main design goal is to improve the throughput of the lattice-based key encapsulation scheme FrodoKEM \cite{frodokem} when implemented in hardware. As described in Section \ref{sec:related}, FrodoKEM is one of the leading conservative candidates submitted to the NIST post-quantum standardisation effort \cite{nistpq}. Moreover, it has been shown to have appealing qualities, SUCH AS which make it an ideal candidate for hardware implementations \cite{howe2018standard}. However a complete exploration of the possible hardware optimization applicable to Frodo is yet to come. For instance, previous implementations do not consider parallelisation or other design alternatives capable of significantly improve the throughput.

As described in Section \ref{sec:related}, FrodoKEM requires heavy use of PRNGs. In the algorithm specifications it is suggest to either use cSHAKE or AES. In particular, the most computationally intensive operations, Line 7 of Algorithm \ref{alg:encaps}, requires $n\times n$ (for $n=640$ or $976$) 16-bit pseudo-random values. To not be the bottle-neck, PRNG needs to achieve an high throughput, typically in the range of COMPLTE HERE. In a previous hardware design, the one proposed by Howe et al. \cite{howe2018standard}, high throughput for the PRNG was achieved by pre-calculating randomness and storing it in BRAM. Random data newly calculated was then written into the memory overwriting the random data previously stored<. This is an efficient approach, however a more efficient PRNG that would not require BRAM usage, would have the potential to increase the operating frequency of the design and thus improve its throughput.

Another issue with the use of AES or cSHAKE is the relatively large area overhead. For example, cSHAKE used within FrodoKEM-640 Encaps occupies 42\% of the overall hardware resources~\cite{howe2018standard}. Bos et al. \cite{cryptoeprint:2018:1116} recently improved the throughput of software implementations of FrodoKEM by leveraging a different PRNG; xoshiro128**. To improve the parallelism of our implementation, we put further this idea to hardware and replace the suggested PRNG. We explored several options and we decided to integrate into our design an unrolled x32 Trivium \cite{de2008trivium} module. This is compatible with the security requirements of the FrodoKEM submission. In fact, the authors of the algorithm suggests that replacing the PRNG with another, that still has good statistical pseudo-random properties, still guarantees the security claims of FrodoKEM. The Trivium architecture we integrate has high throughput and maintains the cryptographic security required in the FrodoKEM specifications, thus perfectly fits our needs. %The primary reason the proposed designs have achieved both high throughput and practical area consumption is through this idea of replacing the PRNG.

\subsection{Hardware Optimisations}

In order to fully explore the potential of FrodoKEM in hardware, we propose several architectures characterized by different design goals (in terms of throughput). We use the proposed architecute to implement key generation, encapsulation, and decapsulation, on both sets of parameters proposed in the specifications: FrodoKEM-640 and FrodoKEM-976. Our designs uses 1x, 4x, 8x, and 16x parallel multiplications during the most computationally intensive parts in FrodoKEM. The following is the LWE calculation of the type: 
\begin{equation}
\mathbf{B} = \mathbf{S} \mathbf{A} + \mathbf{E}, \label{lwe}
\end{equation}
 required in key generation, encapsulation, and decapsulation. It takes approximately 97.5\% of the overall computations \cite{howe2018standard}. As in literature, we exploit DSP slices on the FPGA for the multi-and-accumulate (MAC) operations required for matrix multiplication. Hence, each parallel multiplication of the proposed designs uses its own DSP slice.

The LWE matrix multiplication component incurs in a large computational overhead. Because of this, it is a nice target for optimization. Our optimization heavily relies on parallelization. Firstly we describe the basic LWE multiplier, that includes just one multiplication component. Then we describe how this core is parallelised, allowing us to significantly improve the throughput.

The LWE core is essentially made by vector-matrix multiplication (that is, $\mathbf{S}[\text{row}]\times \mathbf{A}$), addition of an error (that is, $\mathbf{E}$), and, when needed, an addition of the encoding of message data. Since the matrix $\mathbf{S}$ consists of a large number of column entries (either 640 or 976) but only 8 row entries (for both parameter sets), we decided to implement a vector-matrix multiplier, instead of matrix-matrix one. By doing this, we can reuse the same hardware architecture for each row of $\mathbf{S}$, saving significant hardware resources. Each run of the row-column MAC operation exploits a DSP slice on the FPGA, which fits within the 48-bit MAC size of the FPGA. The DSP slice is ideal for these operations, but it also ensures constant computational time, since each multiplication requires one clock cycle. Once each row-column MAC operation is completed, an error value is added from the CDT sampler. These values are consistently added into an instantiation of cSHAKE, which is required to calculate the shared secret, as well as being output as the ciphertext. This process is pipelined to ensure high throughput and constant runtime. A high-level overview of the whole architecture is shown in Figure \ref{arch}.

\begin{figure}[htbp]
    %\advance\leftskip-1cm
\includegraphics[scale=0.8]{figures/arch.pdf}
\caption{A high-level overview of the proposed hardware designs for FrodoKEM for $k$ parallel multipliers.}\label{arch}
\end{figure}

%cSHAKE is still required for RO, this can be done in parallel to the next KEM calculation.

To avoid to use BRAM and while keeping the throughput needed by the MAC operations of the matrix multiplications, the designs require 16 bits of pseduo-randomness per multiplication per clock cycle. Thus, for every two parallel multiplications we require one Trivium instantiation, whose 32-bit output per clock cycle is split up to form two 16-bit pseudo-random integers. This pseudo-randomness forms the matrix $\mathbf{A}$ in Equation \ref{lwe}, whereas the matrix $\mathbf{S}$ and $\mathbf{E}$ require randomness taken from a Gaussian-like distribution. The cumulative distribution table (CDT) sampler technique has been shown to be the most suitable one for hardware. However compared with previous works, we replace the use of AES as a psuedo-random input with Trvium. This ensures the same high throughput, but requires significantly less area on the FPGA.

\begin{figure}[htbp]
    %\advance\leftskip-1cm
\includegraphics[scale=0.5]{figures/parallel.pdf}
\caption{Parallelising matrix multiplication, for $\mathbf{S}\times\mathbf{A}$, used within LWE computations for an example of $k=4$ parallel multiplications.}\label{parallel}
\end{figure}

Overall, the technique we use to parallelise Equation \ref{lwe} is to vertically partition the matrix $\mathbf{A}$ into $k$ equal sections, where $k$ is the number of parallel multiplications used. This is shown in Figure \ref{parallel} for $k=4$ parallel multiplications, utilising 4 DSP slices for MAC. Each vector on the LHS of Figure \ref{parallel} remains the same for each of the $k$ operations. We repeat this vector-matrix operation for the $\bar{n}=8$ rows of the matrix $\mathbf{S}$. This technique is used across all designs for the three cryptographic modules to ensure consistency. %For each multiplication 

In order to produce enough randomness for these multiplications to have no delays, we need one instance of our PRNG, Trivium, for every two parallel multiplications. This because each element of the matrix $\mathbf{A}$ is set to be a 16-bit integer and each output from Trivium is 32 bits, that is, two 16-bit integers. 

\subsection{Efficient First-Order Masking}

We implement first-order masking to the decapsulation operation $\mathbf{M} = \mathbf{C} - \mathbf{B}'\mathbf{S}$, as this is the only instance where secret-key information is used. Our design allows to implement this masking schema without affecting the area consumption or throughput. This is achieved by re-using the optimisations previously discussed. The matrix $\mathbf{S}$ is split using the same technique from Figure \ref{parallel} and our secret shares are generated by re-using the Trivium instances. By computing these calculations in parallel, the masked calculation of $\mathbf{M}$ has the same runtime as the one needed to complete the calculation when masking is not used.




